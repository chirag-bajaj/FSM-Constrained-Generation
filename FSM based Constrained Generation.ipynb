{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSM-based Constrained Generation Example  \n",
    "In this notebook, we demonstrate how to constrain the output of a language model using a finite state machine (FSM).  \n",
    "We’ll use a set of pre-defined HTTP response codes and build an FSM that ensures our language model can only generate one of these valid codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiragbajaj/.virtualenvs/ai-services/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Model and Tokenizer  \n",
    "In this example, we are using the \"Qwen/Qwen2.5-1.5B\" model, but you can replace this with any model available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "torch.manual_seed(42)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Target HTTP Response Codes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_CODES = [\n",
    "    # \"200 OK\",\n",
    "    # \"301 Moved Permanently\",\n",
    "    # \"401 Unauthorized\",\n",
    "    # \"404 Not Found\",\n",
    "    # \"500 Internal Server Error\"\n",
    "    \"200\",\n",
    "    \"301\",\n",
    "    \"401\",\n",
    "    \"404\",\n",
    "    \"500\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the FSM from the HTTP Codes  \n",
    "We create a helper function `build_fsm_for_codes` which converts our HTTP status code strings into sequences of tokenizer token IDs.  \n",
    "For each code, the function builds a chain of states (starting from 0) and records transitions between these states.  \n",
    "Each code’s final state is marked as an accepting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fsm_for_codes(codes, tokenizer):\n",
    "    \"\"\"\n",
    "    For each code string (e.g. \"404 Not Found\"), we:\n",
    "      1) Convert it into a sequence of subword tokens using tokenizer.encode(...).\n",
    "      2) Build a chain of states for each subword token:\n",
    "           - startState --(token1)--> nextState --(token2)-->...--(finalToken)--> acceptState\n",
    "      3) Merge all codes so they share the same startState (0).\n",
    "    \"\"\"\n",
    "    transitions = {}\n",
    "    acceptance_states = set()\n",
    "    start_state = 0\n",
    "    next_free_state = 1\n",
    "\n",
    "    for code_str in codes:\n",
    "        code_token_ids = tokenizer.encode(code_str, add_special_tokens=False)\n",
    "        current_state = start_state\n",
    "\n",
    "        for tid in code_token_ids:\n",
    "            if (current_state, tid) not in transitions:\n",
    "                transitions[(current_state, tid)] = next_free_state\n",
    "                current_state = next_free_state\n",
    "                next_free_state += 1\n",
    "            else:\n",
    "                current_state = transitions[(current_state, tid)]\n",
    "        acceptance_states.add(current_state)\n",
    "\n",
    "    return transitions, acceptance_states, next_free_state - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions, accepting_states, max_state = build_fsm_for_codes(HTTP_CODES, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create the Transition Table  \n",
    "This mapping tells us, for each state in the FSM, which tokens (and corresponding next states) are allowed. It will be used to filter the language model’s output at every generation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_valid_tokens = {s: [] for s in range(max_state + 1)}\n",
    "for (s, tid), ns in transitions.items():\n",
    "    state_to_valid_tokens[s].append((tid, ns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define the Constrained Generation Function  \n",
    "The function `generate_http_code_with_fsm` takes a user prompt, then iteratively guides the language model.  \n",
    "At each step, it restricts the allowed tokens based on the current FSM state. The process stops as soon as it reaches an accepting state or if no valid transitions remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_http_code_with_fsm(user_prompt: str, max_steps=30):\n",
    "    \"\"\"\n",
    "    1) Tokenizes the user prompt.\n",
    "    2) Iteratively generates tokens, filtering them via the FSM transitions.\n",
    "    3) Stops once an accepting state (a complete HTTP code) is reached, or no valid transitions are available.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(user_prompt, return_tensors=\"pt\").input_ids # tokenize the prompt\n",
    "    current_state = 0 \n",
    "    generated_token_ids = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]  # get logits for the last token\n",
    "\n",
    "        valid_info = state_to_valid_tokens.get(current_state, []) # retrieve valid tokens for the current FSM state\n",
    "        if not valid_info:\n",
    "            # if no valid transitions are available, stop generation\n",
    "            break\n",
    "\n",
    "        valid_token_ids = [t[0] for t in valid_info]\n",
    "        next_states = [t[1] for t in valid_info]\n",
    "\n",
    "        # setting logits to -inf basically means setting the token probability to 0\n",
    "        mask = torch.full_like(logits, float('-inf')) \n",
    "        mask[0, valid_token_ids] = logits[0, valid_token_ids] \n",
    "\n",
    "        # applying softmax to get probability as 0 for the invalid tokens\n",
    "        probs = torch.nn.functional.softmax(mask, dim=-1)\n",
    "        num_top = min(3, len(valid_token_ids))\n",
    "        allowed_probs = probs[0, valid_token_ids]\n",
    "        top_probs, top_indices = torch.topk(allowed_probs, k=num_top)\n",
    "        print(f\"Step {step+1}: Top {num_top} allowed tokens:\")\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            token_id = valid_token_ids[idx]\n",
    "            token_str = tokenizer.decode([token_id], clean_up_tokenization_spaces=False)\n",
    "            print(f\"  Token: '{token_str}', Probability: {prob.item():.4f}\")\n",
    "        next_token_id = torch.argmax(probs, dim=-1).item()\n",
    "        # next_token_id = torch.multinomial(probs, num_samples=1).item() # sample the next token\n",
    "\n",
    "        chosen_index = valid_token_ids.index(next_token_id)\n",
    "        current_state = next_states[chosen_index]\n",
    "\n",
    "        generated_token_ids.append(next_token_id)\n",
    "        next_token_tensor = torch.tensor([[next_token_id]])\n",
    "        input_ids = torch.cat([input_ids, next_token_tensor], dim=1) # update input_ids which is used for the next token generation\n",
    "\n",
    "        if current_state in accepting_states:\n",
    "            # if any accepting state is reached, stop generation\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(generated_token_ids, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Constrained Generation  \n",
    "We now test our function with a prompt. The function should output an HTTP response code that follows the constraints of our FSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Top 3 allowed tokens:\n",
      "  Token: '3', Probability: 0.5885\n",
      "  Token: '4', Probability: 0.2437\n",
      "  Token: '2', Probability: 0.0996\n",
      "Step 2: Top 1 allowed tokens:\n",
      "  Token: '0', Probability: 1.0000\n",
      "Step 3: Top 1 allowed tokens:\n",
      "  Token: '1', Probability: 1.0000\n",
      "\n",
      "HTTP status code (three digits only) for a site that has moved permanently:\n",
      "301\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"HTTP status code (three digits only) for a site that has moved permanently:\"\n",
    "# user_prompt = \"Enter the three-digit HTTP response code for a permanent redirect: \"\n",
    "# user_prompt = \"The following HTTP status code should consist solely of three digits. For a successful request, type: \"\n",
    "# user_prompt = \"Kindly supply only the numerical three-digit HTTP status code for a resource that has been moved permanently: \"\n",
    "# user_prompt = \"HTTP code: Only a three-digit number. What code indicates a site not found error? \"\n",
    "# user_prompt = \"What is the three-digit numeric HTTP status code for unauthorized access? \"\n",
    "# user_prompt = \"When the server experiences an unexpected error during processing, it returns a specific three-digit code. Provide that code for an internal server error: \"\n",
    "\n",
    "output_text = generate_http_code_with_fsm(user_prompt)\n",
    "print(f\"\\n{user_prompt}\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Hallucinations in language models are instances where the generated output includes information or tokens that don't strictly follow the input or expected constraints. Even under strict mechanisms like FSM-constrained decoding, hallucinations can occur due to several factors:\n",
    "* The model’s internal probability distribution might weigh context, learned patterns, and prompt cues in unexpected ways, causing it to occasionally favor a token sequence that diverges from your intended output.\n",
    "* Strong constraints can conflict with the natural flow of language that the model was trained on—if the allowed tokens don’t match the model’s unconstrained output distribution well, the decoder might produce extra tokens or slightly modify the intended answer.\n",
    "* The prompt context might not perfectly isolate the constrained part, leading the model to “hallucinate” additional context before or after the desired three-digit code.\n",
    "* Even small variations in tokenization can affect how the FSM maps allowed transitions to tokens, creating scenarios where the model’s logits and hard constraints interact unpredictably."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-services",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
